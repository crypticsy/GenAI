{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crypticsy/anaconda3/envs/genai/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import error: No module named 'triton'\n"
     ]
    }
   ],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\n",
      "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
      " Where:\n",
      "TP: True positive\n",
      "TN: True negative\n",
      "FP: False positive\n",
      "FN: False negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "print(accuracy.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': Value(dtype='int32', id=None),\n",
       " 'references': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Label mapping\n",
    "label_map = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "\n",
    "# Create the classifier pipeline\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    tokenizer=\"distilbert-base-uncased\",\n",
    ")\n",
    "\n",
    "# Input texts\n",
    "texts = [\n",
    "    \"I love this movie.\",\n",
    "    \"This movie was terrible.\",\n",
    "    \"I don't like this movie.\",\n",
    "    \"This movie was great!\",\n",
    "]\n",
    "\n",
    "# Run the classifier\n",
    "outputs = classifier(texts, truncation=True)\n",
    "\n",
    "# Convert predicted labels to integers\n",
    "predicted_labels = [label_map[output[\"label\"]] for output in outputs]\n",
    "\n",
    "# Ground truth labels (also mapped to integers)\n",
    "true_labels = [label_map[label] for label in [\"POSITIVE\", \"NEGATIVE\", \"NEGATIVE\", \"POSITIVE\"]]\n",
    "\n",
    "# Load metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Compute metrics\n",
    "accuracy_score = accuracy.compute(predictions=predicted_labels, references=true_labels)\n",
    "precision_score = precision.compute(predictions=predicted_labels, references=true_labels)\n",
    "recall_score = recall.compute(predictions=predicted_labels, references=true_labels)\n",
    "f1_score = f1.compute(predictions=predicted_labels, references=true_labels)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy_score['accuracy']}\")\n",
    "print(f\"Precision: {precision_score['precision']}\")\n",
    "print(f\"Recall: {recall_score['recall']}\")\n",
    "print(f\"F1: {f1_score['f1']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "### Perplexity\n",
    "- A model's ability to predict the next word accurately and confidently.\n",
    "- Lower perplexity indicates better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Latest research findings in Antartica show\"\n",
    "\n",
    "generated_text = \"Latest research findings in Antartica show that the ice is melting faster than previously thought.\"\n",
    "\n",
    "# Encode the prompt, generate the text, and decode it\n",
    "input_text_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "output = model.generate(input_text_ids, max_length=50)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "results = perplexity.compute(\n",
    "    predictions=[generated_text],\n",
    "    references=[input_text],\n",
    ")\n",
    "\n",
    "print(f\"Perplexity: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "### BLEU\n",
    "- Measures translation quality by comparing machine-generated text to human references.\n",
    "- Predictions: LLM's outputs.\n",
    "- References: Human-generated translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 1.0,\n",
       " 'precisions': [1.0, 1.0, 1.0, 1.0],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.2142857142857142,\n",
       " 'translation_length': 17,\n",
       " 'reference_length': 14}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "input_text = \"Latest research findings in Antartica show\"\n",
    "\n",
    "references = [[\n",
    "    \"Latest research findings in Antartica show significant ice loss due to climate change.\",\n",
    "    \"Latest research findings in Antartica show that the ice sheet is melting faster than previously thought.\"\n",
    "]]\n",
    "\n",
    "generated_text = \"Latest research findings in Antartica show that the ice sheet is melting faster than previously thought.\"\n",
    "\n",
    "results = bleu.compute(\n",
    "    predictions=[generated_text],\n",
    "    references=references,\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "## Summarization\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "### ROUGE\n",
    "- Similarity between generated summaries and reference summaries.\n",
    "  - Looks at n-grams and overlapping\n",
    "  - predictions = LLM's outputs\n",
    "  - references = human-provided summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(0.7906976744186046), 'rouge2': np.float64(0.5365853658536585), 'rougeL': np.float64(0.7441860465116279), 'rougeLsum': np.float64(0.7441860465116279)}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "predictions = [\n",
    "  \"\"\"as we learn more about the frequency and size distribution of exoplanets, we are discovering that terrestrial planets are exceedingly common\"\"\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "  \"\"\"The more we learn about the frequency and size distribution of exoplanets, the more confident we are that they are exceedingly common\"\"\"\n",
    "]\n",
    "\n",
    "results = rouge.compute(\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "## Translation\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "### METEOR\n",
    "- more linguistic features like word variations, similar meanings, and word order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 7.02k/7.02k [00:00<00:00, 23.6MB/s]\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/crypticsy/nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/crypticsy/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/crypticsy/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "blue = evaluate.load(\"bleu\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "predictions = [\n",
    "  \"He thought it right and necessary to become a knight-errant, roaming the world in armor, seeking adventures and practicing the deds he had read about in chivalric tales.\"\n",
    "]\n",
    "\n",
    "reference = [\n",
    "  \"He believed it was proper and essential to transform into a knight-errant, traveling the world in armor, pursuing adventures, and enacting the heroic deeds he had encountered in tales of chivalry.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.1724871568079977\n",
      "METEOR: 0.49634304588887584\n"
     ]
    }
   ],
   "source": [
    "results_bleu = bleu.compute(predictions=predictions, references=reference)\n",
    "results_meteor = meteor.compute(predictions=predictions, references=reference)\n",
    "\n",
    "print(f\"BLEU: {results_bleu['bleu']}\")\n",
    "print(f\"METEOR: {results_meteor['meteor']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "## Question Answering\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "### Exact Match (EM)\n",
    "- 1 if an LLM's output exactly matches the reference answer, 0 otherwise.\n",
    "- Normally used in conjunction with F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 5.67k/5.67k [00:00<00:00, 15.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match: 0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "em_metric = evaluate.load(\"exact_match\")\n",
    "\n",
    "predictions = [\n",
    "  \"The cat sat on the mat.\",\n",
    "  \"Theaters are great.\",\n",
    "  \"Like comparing oranges and apples.\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "  \"The cat sat on the mat?\",\n",
    "  \"Theaters are great.\",\n",
    "  \"Like comparing apples and oranges.\"\n",
    "]\n",
    "\n",
    "results = em_metric.compute(\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    ")\n",
    "\n",
    "print(f\"Exact Match: {results['exact_match']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "## Metrics for analyzing LLMs bias\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "### Toxicity\n",
    "- quantifies text toxicity using pre-trained hate speech classifier\n",
    "  - aggregation=\"maximum\" returns maximum toxicity score across all sentences\n",
    "  - aggregation=\"ratio\" returns % of predictions that are toxic above 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate_modules.metrics.evaluate-measurement--toxicity.2390290fa0bf6d78480143547c6b08f3d4f8805b249df8c7a8e80d0ce8e3778b.toxicity:Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity for texts_1: {'max_toxicity': 0.00013866178051102906}\n",
      "Toxicity for texts_2: {'max_toxicity': 0.0268279816955328}\n"
     ]
    }
   ],
   "source": [
    "toxicity_metric = evaluate.load(\"toxicity\")\n",
    "\n",
    "texts_1 = [\"Everyone in the team adores him\", \"He is a true genius, pure talent\"]\n",
    "texts_2 = [\"Nobody in the team likes him\", \"He is useless, 'good-for-nothing'\"]\n",
    "\n",
    "toxicity_results_1 = toxicity_metric.compute(predictions=texts_1, aggregation=\"maximum\")\n",
    "toxicity_results_2 = toxicity_metric.compute(predictions=texts_2, aggregation=\"maximum\")\n",
    "\n",
    "print(f\"Toxicity for texts_1: {toxicity_results_1}\")\n",
    "print(f\"Toxicity for texts_2: {toxicity_results_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "### Regard\n",
    "- language polarity and biased pereption towards certain demographics\n",
    "- evaluate regard scores on LLM outputs associated to two groups separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity for group1: {'regard': [[{'label': 'positive', 'score': 0.9098386764526367}, {'label': 'neutral', 'score': 0.05939692258834839}, {'label': 'other', 'score': 0.026468118652701378}, {'label': 'negative', 'score': 0.004296257160604}], [{'label': 'positive', 'score': 0.7809811234474182}, {'label': 'neutral', 'score': 0.18085995316505432}, {'label': 'other', 'score': 0.030492888763546944}, {'label': 'negative', 'score': 0.007666007615625858}]]}\n"
     ]
    }
   ],
   "source": [
    "regard = evaluate.load(\"regard\")\n",
    "regard_comp = evaluate.load(\"regard\", \"compare\")\n",
    "\n",
    "group1 = [\n",
    "  \"abc are described as loyal employees\",\n",
    "  \"abc are ambitious in their career expectations\",\n",
    "]\n",
    "\n",
    "group2 = [\n",
    "  \"abc are known for causing lots of team conflicts\",\n",
    "  \"abc are verbally violent\"\n",
    "]\n",
    "\n",
    "polarity_results_1 = regard.compute(data=group1)\n",
    "polarity_results_2 = regard.compute(data=group2)\n",
    "\n",
    "polarity_results_comp = regard_comp.compute(data=group1, references=group2)\n",
    "print(f\"Polarity for group1: {polarity_results_1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
