{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Fine-tuning Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TorchTune for Llama fine-tuning\n",
    "- Based on configurable templates\n",
    "- Ideal for: scaling quickly\n",
    "<br/><br/>\n",
    "\n",
    "#### SFTTrainer from Hugging Face\n",
    "- Access to other LLMs\n",
    "- Ideal for: fine-tuning multiple models\n",
    "<br/><br/>\n",
    "\n",
    "#### Unsloth\n",
    "- Efficient memory usage\n",
    "- Ideal for: limited hardware\n",
    "<br/><br/>\n",
    "\n",
    "#### Axolotl\n",
    "- Modular approach\n",
    "- Ideal for: no extensive reconfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TorchTune Receipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import error: No module named 'triton'\n",
      "W0318 09:24:16.595000 44671 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "RECIPE                                   CONFIG                                  \n",
      "full_finetune_single_device              llama2/7B_full_low_memory               \n",
      "                                         code_llama2/7B_full_low_memory          \n",
      "                                         llama3/8B_full_single_device            \n",
      "                                         llama3_1/8B_full_single_device          \n",
      "                                         llama3_2/1B_full_single_device          \n",
      "                                         llama3_2/3B_full_single_device          \n",
      "                                         mistral/7B_full_low_memory              \n",
      "                                         phi3/mini_full_low_memory               \n",
      "                                         qwen2/7B_full_single_device             \n",
      "                                         qwen2/0.5B_full_single_device           \n",
      "                                         qwen2/1.5B_full_single_device           \n",
      "                                         qwen2_5/0.5B_full_single_device         \n",
      "                                         qwen2_5/1.5B_full_single_device         \n",
      "                                         qwen2_5/3B_full_single_device           \n",
      "                                         qwen2_5/7B_full_single_device           \n",
      "                                         llama3_2_vision/11B_full_single_device  \n",
      "full_finetune_distributed                llama2/7B_full                          \n",
      "                                         llama2/13B_full                         \n",
      "                                         llama3/8B_full                          \n",
      "                                         llama3_1/8B_full                        \n",
      "                                         llama3_2/1B_full                        \n",
      "                                         llama3_2/3B_full                        \n",
      "                                         llama3/70B_full                         \n",
      "                                         llama3_1/70B_full                       \n",
      "                                         llama3_3/70B_full                       \n",
      "                                         mistral/7B_full                         \n",
      "                                         gemma/2B_full                           \n",
      "                                         gemma/7B_full                           \n",
      "                                         gemma2/2B_full                          \n",
      "                                         gemma2/9B_full                          \n",
      "                                         gemma2/27B_full                         \n",
      "                                         phi3/mini_full                          \n",
      "                                         qwen2/7B_full                           \n",
      "                                         qwen2/0.5B_full                         \n",
      "                                         qwen2/1.5B_full                         \n",
      "                                         qwen2_5/0.5B_full                       \n",
      "                                         qwen2_5/1.5B_full                       \n",
      "                                         qwen2_5/3B_full                         \n",
      "                                         qwen2_5/7B_full                         \n",
      "                                         llama3_2_vision/11B_full                \n",
      "                                         llama3_2_vision/90B_full                \n",
      "lora_finetune_single_device              llama2/7B_lora_single_device            \n",
      "                                         llama2/7B_qlora_single_device           \n",
      "                                         code_llama2/7B_lora_single_device       \n",
      "                                         code_llama2/7B_qlora_single_device      \n",
      "                                         llama3/8B_lora_single_device            \n",
      "                                         llama3_1/8B_lora_single_device          \n",
      "                                         llama3/8B_qlora_single_device           \n",
      "                                         llama3_2/1B_lora_single_device          \n",
      "                                         llama3_2/3B_lora_single_device          \n",
      "                                         llama3/8B_dora_single_device            \n",
      "                                         llama3/8B_qdora_single_device           \n",
      "                                         llama3_1/8B_qlora_single_device         \n",
      "                                         llama3_2/1B_qlora_single_device         \n",
      "                                         llama3_2/3B_qlora_single_device         \n",
      "                                         llama2/13B_qlora_single_device          \n",
      "                                         mistral/7B_lora_single_device           \n",
      "                                         mistral/7B_qlora_single_device          \n",
      "                                         gemma/2B_lora_single_device             \n",
      "                                         gemma/2B_qlora_single_device            \n",
      "                                         gemma/7B_lora_single_device             \n",
      "                                         gemma/7B_qlora_single_device            \n",
      "                                         gemma2/2B_lora_single_device            \n",
      "                                         gemma2/2B_qlora_single_device           \n",
      "                                         gemma2/9B_lora_single_device            \n",
      "                                         gemma2/9B_qlora_single_device           \n",
      "                                         gemma2/27B_lora_single_device           \n",
      "                                         gemma2/27B_qlora_single_device          \n",
      "                                         phi3/mini_lora_single_device            \n",
      "                                         phi3/mini_qlora_single_device           \n",
      "                                         qwen2/7B_lora_single_device             \n",
      "                                         qwen2/0.5B_lora_single_device           \n",
      "                                         qwen2/1.5B_lora_single_device           \n",
      "                                         qwen2_5/0.5B_lora_single_device         \n",
      "                                         qwen2_5/1.5B_lora_single_device         \n",
      "                                         qwen2_5/3B_lora_single_device           \n",
      "                                         qwen2_5/7B_lora_single_device           \n",
      "                                         qwen2_5/14B_lora_single_device          \n",
      "                                         llama3_2_vision/11B_lora_single_device  \n",
      "                                         llama3_2_vision/11B_qlora_single_device \n",
      "lora_dpo_single_device                   llama2/7B_lora_dpo_single_device        \n",
      "                                         llama3_1/8B_lora_dpo_single_device      \n",
      "lora_dpo_distributed                     llama2/7B_lora_dpo                      \n",
      "                                         llama3_1/8B_lora_dpo                    \n",
      "ppo_full_finetune_single_device          mistral/7B_full_ppo_low_memory          \n",
      "lora_finetune_distributed                llama2/7B_lora                          \n",
      "                                         llama2/13B_lora                         \n",
      "                                         llama2/70B_lora                         \n",
      "                                         llama2/7B_qlora                         \n",
      "                                         llama2/70B_qlora                        \n",
      "                                         llama3/8B_dora                          \n",
      "                                         llama3/70B_lora                         \n",
      "                                         llama3_1/70B_lora                       \n",
      "                                         llama3_3/70B_lora                       \n",
      "                                         llama3_3/70B_qlora                      \n",
      "                                         llama3/8B_lora                          \n",
      "                                         llama3_1/8B_lora                        \n",
      "                                         llama3_2/1B_lora                        \n",
      "                                         llama3_2/3B_lora                        \n",
      "                                         llama3_1/405B_qlora                     \n",
      "                                         mistral/7B_lora                         \n",
      "                                         gemma/2B_lora                           \n",
      "                                         gemma/7B_lora                           \n",
      "                                         gemma2/2B_lora                          \n",
      "                                         gemma2/9B_lora                          \n",
      "                                         gemma2/27B_lora                         \n",
      "                                         phi3/mini_lora                          \n",
      "                                         qwen2/7B_lora                           \n",
      "                                         qwen2/0.5B_lora                         \n",
      "                                         qwen2/1.5B_lora                         \n",
      "                                         qwen2_5/0.5B_lora                       \n",
      "                                         qwen2_5/1.5B_lora                       \n",
      "                                         qwen2_5/3B_lora                         \n",
      "                                         qwen2_5/7B_lora                         \n",
      "                                         qwen2_5/32B_lora                        \n",
      "                                         qwen2_5/72B_lora                        \n",
      "                                         llama3_2_vision/11B_lora                \n",
      "                                         llama3_2_vision/11B_qlora               \n",
      "                                         llama3_2_vision/90B_lora                \n",
      "                                         llama3_2_vision/90B_qlora               \n",
      "generate                                 generation                              \n",
      "dev/generate_v2                          llama2/generation_v2                    \n",
      "                                         llama3_2_vision/11B_generation_v2       \n",
      "dev/early_exit_finetune_distributed      llama2/7B_full_early_exit               \n",
      "eleuther_eval                            eleuther_evaluation                     \n",
      "                                         llama3_2_vision/11B_evaluation          \n",
      "                                         qwen2/evaluation                        \n",
      "                                         gemma/evaluation                        \n",
      "                                         phi3/evaluation                         \n",
      "                                         mistral/evaluation                      \n",
      "quantize                                 quantization                            \n",
      "qat_distributed                          llama2/7B_qat_full                      \n",
      "                                         llama3/8B_qat_full                      \n",
      "qat_lora_finetune_distributed            llama3/8B_qat_lora                      \n",
      "                                         llama3_1/8B_qat_lora                    \n",
      "                                         llama3_2/1B_qat_lora                    \n",
      "                                         llama3_2/3B_qat_lora                    \n",
      "knowledge_distillation_single_device     qwen2/1.5_to_0.5B_KD_lora_single_device \n",
      "                                         llama3_2/8B_to_1B_KD_lora_single_device \n",
      "knowledge_distillation_distributed       qwen2/1.5_to_0.5B_KD_lora_distributed   \n",
      "                                         llama3_2/8B_to_1B_KD_lora_distributed   \n"
     ]
    }
   ],
   "source": [
    "!tune ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of running file-tuning\n",
    "# !tune run full_finetune_single_device --config \\llama3_1/8B_lora_single_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset from Hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flags', 'instruction', 'category', 'intent', 'response']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "ds = load_dataset(\n",
    "  \"bitext/Bitext-customer-support-llm-chatbot-training-dataset\",\n",
    "  split=\"train\"\n",
    ")\n",
    "\n",
    "print(ds.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peeking into the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 'ORDER',\n",
      " 'flags': 'B',\n",
      " 'instruction': 'question about cancelling order {{Order Number}}',\n",
      " 'intent': 'cancel_order',\n",
      " 'response': \"I've understood you have a question regarding canceling order \"\n",
      "             \"{{Order Number}}, and I'm here to provide you with the \"\n",
      "             'information you need. Please go ahead and ask your question, and '\n",
      "             \"I'll do my best to assist you.\"}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['flags', 'instruction', 'category', 'intent', 'response'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_thousand_points = ds[:1000]\n",
    "filtered_ds = Dataset.from_dict(first_thousand_points)\n",
    "\n",
    "filtered_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 27914.01 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query : question about cancelling order {{Order Number}}\n",
      "Response : I've understood you have a question regarding canceling order {{Order Number}}, and I'm here to provide you with the information you need. Please go ahead and ask your question, and I'll do my best to assist you.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def merge_example(row):\n",
    "  row['conversation'] = f\"Query : {row['instruction']}\\nResponse : {row['response']}\"\n",
    "  return row\n",
    "\n",
    "filtered_ds = filtered_ds.map(merge_example)\n",
    "print(filtered_ds[0][\"conversation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 194513.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_ds.save_to_disk(\"preprocessed_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query : question about cancelling order {{Order Number}}\n",
      "Response : I've understood you have a question regarding canceling order {{Order Number}}, and I'm here to provide you with the information you need. Please go ahead and ask your question, and I'll do my best to assist you.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds_preprocessed = load_from_disk(\"preprocessed_dataset\")\n",
    "print(ds_preprocessed[0][\"conversation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning with torchtune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Receipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_dict = {\n",
    "    \"model\":{\n",
    "        \"_component_\" : \"torchtune.models.llama3_2.llama3_2_1b\"\n",
    "    },\n",
    "    \"batch_size\":8,\n",
    "    \"device\":\"cuda\",\n",
    "    \"epochs\": 15,\n",
    "    \"optimizer\": {\"_component_\": \"bitsandbytes.optim.PagedAdamW8bit\", \"lr\": 3e-05},\n",
    "    \"dataset\": {\"_component_\": \"custom_dataset\"},\n",
    "    \"output_dir\": \"/tmp/finetune_results\"\n",
    "}\n",
    "\n",
    "yaml_file_path = \"custom_receipe.yaml\"\n",
    "with open(yaml_file_path, \"w\") as file:\n",
    "  yaml.dump(config_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tune run --config custom_receipe.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
