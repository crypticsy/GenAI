{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1291c4-61f8-49ef-899f-89c178cdfd58",
   "metadata": {},
   "source": [
    "![email inbox](email_inbox.jpg)\n",
    "\n",
    "Every day, professionals wade through hundreds of emails, from urgent client requests to promotional offers. It's like trying to find important messages in a digital ocean. But AI can help you stay afloat by automatically sorting emails to highlight what matters most.\n",
    "\n",
    "You've been asked to build an intelligent email assistant using Llama, to help users automatically classify their incoming emails. Your system will identify which emails need immediate attention, which are regular updates, and which are promotions that can wait or be archived.\n",
    "\n",
    "### The Data\n",
    "You'll work with a dataset of various email examples, ranging from urgent business communications to promotional offers. Here's a peek at what you'll be working with:\n",
    "\n",
    "### email_categories_data.csv\n",
    "\n",
    " Column | Description |\n",
    "|--------|-------------|\n",
    "| email_id | A unique identifier for each email in the dataset. |\n",
    "| email_content | The full email text including subject line and body. Each email follows a format of \"Subject\" followed by the message content on a new line. |\n",
    "| expected_category | The correct classification of the email: `Priority`, `Updates`, or `Promotions`. This will be used to validate your model's performance. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c398267d-bd03-485d-80af-db6d6d2529e4",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 3118,
    "lastExecutedAt": 1742289042048,
    "lastExecutedByKernel": "6bc902cb-b348-496e-99fa-74a856df6f7d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Run the following cells first\n# Install necessary packages\n!pip install llama-cpp-python==0.2.82 -q -q -q",
    "outputsMetadata": {
     "0": {
      "height": 353,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Run the following cells first\n",
    "# Install necessary packages\n",
    "!pip install llama-cpp-python==0.2.82 -q -q -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94343497-bed2-4a56-b36a-37dadde55a52",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 8357,
    "lastExecutedAt": 1742289050407,
    "lastExecutedByKernel": "6bc902cb-b348-496e-99fa-74a856df6f7d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Download the model\n!wget -q https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF/resolve/main/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf?download=true -O model.gguf"
   },
   "outputs": [],
   "source": [
    "# Download the model\n",
    "!wget -q https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF/resolve/main/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf?download=true -O model.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6932fc36-bb8e-4ae3-86fd-e3a677a26b4e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1742289050458,
    "lastExecutedByKernel": "6bc902cb-b348-496e-99fa-74a856df6f7d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import required libraries\nimport pandas as pd\nfrom llama_cpp import Llama"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5fea094-0942-4abb-aa0b-c78bb3d2543c",
   "metadata": {
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 47,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1742289050506,
    "lastExecutedByKernel": "6bc902cb-b348-496e-99fa-74a856df6f7d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Load the email dataset\nemails_df = pd.read_csv('data/email_categories_data.csv')\n# Display the first few rows of our dataset\nprint(\"Preview of our email dataset:\")\nemails_df.head(2)",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     },
     "1": {
      "height": 550,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "8c937638-e007-40ba-a407-cd3092d5f50a",
        "nodeType": "const"
       },
       "quickFilterText": ""
      },
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of our email dataset:\n"
     ]
    },
    {
     "data": {
      "application/com.datacamp.data-table.v2+json": {
       "table": {
        "data": {
         "email_content": [
          "Urgent: Server Maintenance Required\\nOur main server needs immediate maintenance due to critical errors. Please address ASAP.",
          "50% Off Spring Collection!\\nDon't miss our biggest sale of the season! All spring items half off. Limited time offer."
         ],
         "email_id": [
          1,
          2
         ],
         "expected_category": [
          "Priority",
          "Promotions"
         ],
         "index": [
          0,
          1
         ]
        },
        "schema": {
         "fields": [
          {
           "name": "index",
           "type": "integer"
          },
          {
           "name": "email_id",
           "type": "integer"
          },
          {
           "name": "email_content",
           "type": "string"
          },
          {
           "name": "expected_category",
           "type": "string"
          }
         ],
         "pandas_version": "1.4.0",
         "primaryKey": [
          "index"
         ]
        }
       },
       "total_rows": 2,
       "truncation_type": null
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_id</th>\n",
       "      <th>email_content</th>\n",
       "      <th>expected_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Urgent: Server Maintenance Required\\nOur main ...</td>\n",
       "      <td>Priority</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>50% Off Spring Collection!\\nDon't miss our big...</td>\n",
       "      <td>Promotions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   email_id  ... expected_category\n",
       "0         1  ...          Priority\n",
       "1         2  ...        Promotions\n",
       "\n",
       "[2 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "application/com.datacamp.data-table.v2+json": {
       "status": "success"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the email dataset\n",
    "emails_df = pd.read_csv('data/email_categories_data.csv')\n",
    "# Display the first few rows of our dataset\n",
    "print(\"Preview of our email dataset:\")\n",
    "emails_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ade0f6f-e0db-415d-b906-e9820370b34e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 303,
    "lastExecutedAt": 1742289050810,
    "lastExecutedByKernel": "6bc902cb-b348-496e-99fa-74a856df6f7d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Initialize the Llama model\nllm = Llama(\"model.gguf\")",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 201 tensors from model.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = py007_tinyllama-1.1b-chat-v0.3\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 22\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32003]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32003]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32003]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   45 tensors\n",
      "llama_model_loader: - type q4_K:  135 tensors\n",
      "llama_model_loader: - type q6_K:   21 tensors\n",
      "llm_load_vocab: special tokens cache size = 262\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32003\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 22\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 5632\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 1.10 B\n",
      "llm_load_print_meta: model size       = 636.18 MiB (4.85 BPW) \n",
      "llm_load_print_meta: general.name     = py007_tinyllama-1.1b-chat-v0.3\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32002 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.09 MiB\n",
      "llm_load_tensors:        CPU buffer size =   636.18 MiB\n",
      "......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    11.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    66.51 MiB\n",
      "llama_new_context_with_model: graph nodes  = 710\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '2048', 'general.name': 'py007_tinyllama-1.1b-chat-v0.3', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '5632', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '64', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '22', 'llama.attention.head_count_kv': '4', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Llama model\n",
    "llm = Llama(\"model.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03ed7d4c-6824-4c42-bce2-8a3ad0ddf96b",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1742289050858,
    "lastExecutedByKernel": "6bc902cb-b348-496e-99fa-74a856df6f7d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Design the prompt\nprompt = \"\"\"\nYou need to classify the emails into three category: Priority, Updates and Promotions.\n\nEmail 1: Important - Billing Issue. Your payment failed. Please update your billing details immediately.\nCategory: Priority\n\nEmail 2: We’ve got some exciting news! We’re always working to bring you the best, and we’re thrilled to share our latest product.\nCategory: Updates\n\nEmail 3: Limited-Time Offer! 🎉 Get up to 30% OFF on our top-quality product this summer!\nCategory: Promotions\n\n\"\"\""
   },
   "outputs": [],
   "source": [
    "# Design the prompt\n",
    "prompt = \"\"\"\n",
    "You need to classify the emails into three category: Priority, Updates and Promotions.\n",
    "\n",
    "Email 1: Important - Billing Issue. Your payment failed. Please update your billing details immediately.\n",
    "Category: Priority\n",
    "\n",
    "Email 2: We’ve got some exciting news! We’re always working to bring you the best, and we’re thrilled to share our latest product.\n",
    "Category: Updates\n",
    "\n",
    "Email 3: Limited-Time Offer! 🎉 Get up to 30% OFF on our top-quality product this summer!\n",
    "Category: Promotions\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e643597b-ec40-4f89-8052-a53ad86bb9ac",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 55,
    "lastExecutedAt": 1742289050914,
    "lastExecutedByKernel": "6bc902cb-b348-496e-99fa-74a856df6f7d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Processing Messages\ndef process_message(llm, user_prompt, message):\n    input_prompt = user_prompt + f\"\"\"Email 4: {message}\n    Category:\"\"\"\n    \n    output = llm(\n        input_prompt,\n        max_tokens=5,\n        temperature=0,\n        stop=[\"Example\", \"\\n\"]\n    )\n    \n    return output[\"choices\"][0][\"text\"].strip()"
   },
   "outputs": [],
   "source": [
    "# Processing Messages\n",
    "def process_message(llm, user_prompt, message):\n",
    "    input_prompt = user_prompt + f\"\"\"Email 4: {message}\n",
    "    Category:\"\"\"\n",
    "    \n",
    "    output = llm(\n",
    "        input_prompt,\n",
    "        max_tokens=5,\n",
    "        temperature=0,\n",
    "        stop=[\"Example\", \"\\n\"]\n",
    "    )\n",
    "    \n",
    "    return output[\"choices\"][0][\"text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "416c3ba9-ecc8-4af8-87d7-d4c3e7c66f47",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 14088,
    "lastExecutedAt": 1742289065004,
    "lastExecutedByKernel": "6bc902cb-b348-496e-99fa-74a856df6f7d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Testing the model\nresults = []\n\nfor i in range(2):\n    message = emails_df.loc[i][\"email_content\"]\n    \n    print(\"Message:\", message)\n    print(\"Expected Category:\", emails_df.loc[i][\"expected_category\"])\n    \n    results.append(process_message(llm, prompt, message))\n    print(\"Predicted Category:\", results[-1])",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     },
     "1": {
      "height": 164,
      "type": "stream"
     },
     "2": {
      "height": 80,
      "type": "stream"
     },
     "3": {
      "height": 143,
      "type": "stream"
     },
     "4": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Urgent: Server Maintenance Required\\nOur main server needs immediate maintenance due to critical errors. Please address ASAP.\n",
      "Expected Category: Priority\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   10073.80 ms\n",
      "llama_print_timings:      sample time =       0.49 ms /     3 runs   (    0.16 ms per token,  6147.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10073.58 ms /   176 tokens (   57.24 ms per token,    17.47 tokens per second)\n",
      "llama_print_timings:        eval time =     209.45 ms /     2 runs   (  104.73 ms per token,     9.55 tokens per second)\n",
      "llama_print_timings:       total time =   10285.91 ms /   178 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: Priority\n",
      "Message: 50% Off Spring Collection!\\nDon't miss our biggest sale of the season! All spring items half off. Limited time offer.\n",
      "Expected Category: Promotions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   10073.80 ms\n",
      "llama_print_timings:      sample time =       0.46 ms /     3 runs   (    0.15 ms per token,  6479.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3404.23 ms /    34 tokens (  100.12 ms per token,     9.99 tokens per second)\n",
      "llama_print_timings:        eval time =     208.96 ms /     2 runs   (  104.48 ms per token,     9.57 tokens per second)\n",
      "llama_print_timings:       total time =    3614.79 ms /    36 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: Clothing\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "results = []\n",
    "\n",
    "for i in range(2):\n",
    "    message = emails_df.loc[i][\"email_content\"]\n",
    "    \n",
    "    print(\"Message:\", message)\n",
    "    print(\"Expected Category:\", emails_df.loc[i][\"expected_category\"])\n",
    "    \n",
    "    results.append(process_message(llm, prompt, message))\n",
    "    print(\"Predicted Category:\", results[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c12819f9-1041-4752-b7e4-3071b08f60a3",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 50,
    "lastExecutedAt": 1742289065054,
    "lastExecutedByKernel": "6bc902cb-b348-496e-99fa-74a856df6f7d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "result1, result2 = results\n\nresult1, result2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Priority', 'Clothing')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1, result2 = results\n",
    "\n",
    "result1, result2"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
