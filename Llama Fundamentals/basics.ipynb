{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Llama class\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_path = \"models/llama_model\"\n",
    "\n",
    "# Initialize the Llama model\n",
    "llm = Llama(model_path=llama_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the most used database for data storage?\"\n",
    "\n",
    "response = llm(\n",
    "  question,\n",
    "  \n",
    "  # Tuning parameters \n",
    "  temperature=0.5,      # Higher temperature means more creative answers, 0 to 1\n",
    "  top_k=20,             # Higher top_k means more diverse response, 1 to infinity\n",
    "  top_p=0.4,            # Lower top_p means more focused response, 0 to 1\n",
    "  max_tokens=100,       # Maximum number of tokens in the response to define the type of response (short, medium, long), 1 to infinity\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first choice and generated text\n",
    "extracted = response[\"choices\"][0][\"text\"]\n",
    "\n",
    "# Print the extracted text\n",
    "print(extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roles for the Llama model\n",
    "# System : Sets the personality and style of the llama\n",
    "# User : Represents the person asking the questions\n",
    "\n",
    "system_message = \"You are a business consultant who gives data-driven answers.\"\n",
    "user_message = \"What are the key factors in a successful marketing strategy?\"\n",
    "\n",
    "message_list = [\n",
    "  {\n",
    "   \"role\" : \"system\",\n",
    "    \"content\" : system_message \n",
    "  },\n",
    "  {\n",
    "    \"role\" : \"user\",\n",
    "    \"content\" : user_message\n",
    "  }\n",
    "]\n",
    "\n",
    "response = llm.create_chat_completion(\n",
    "  messages = message_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zero-shot prompting\n",
    "\n",
    "A single instruction to the model to perform a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Summarize recent mergers in the airline industry.\"\n",
    "output = llm(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add formatting to the prompt\n",
    "prompt=\"\"\"\n",
    "Instruction: Explain the concept of gravity in simple terms.\n",
    "Question: What is gravity?\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "#Â Send the prompt to the model\n",
    "output = llm(prompt, max_tokens=15, stop=[\"Question:\"]) \n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Few-shot prompting\n",
    "\n",
    "A few examples to the model to perform a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Aircraft Model: Boeing 787-9\n",
    "Passenger Capacity: 296\n",
    "Fuel Consumption: 2.5 liters per seat per 100 km\n",
    "\n",
    "Aircraft Model: Airbus A321XLR\n",
    "Passenger Capacity: 244\n",
    "Fuel Consumption: 2.9 liters per seat per 100 km\n",
    "\"\"\"\n",
    "\n",
    "output = llm(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Stop Words\n",
    "\n",
    "Question answering application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Which airlines operate direct flights from London to Singapore?\"\n",
    "\n",
    "output = llm(text, stop=[\"Q:\"]) # Stop words are used to separate the question from the answer\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the few-shot prompt\n",
    "prompt=\"\"\"Review 1: I ordered from this place last night, and I'm impressed! \n",
    "Sentiment 1: Positive,\n",
    "Review 2: My order was delayed by over an hour without any updates. Disappointing!  \n",
    "Sentiment 2: Negative,\n",
    "Review 3: The food quality is top-notch. Highly recommend! \n",
    "Sentiment 3: Positive,\n",
    "Review 4: Delicious food, and excellent customer service! \n",
    "Sentiment 4:\"\"\"\n",
    "\n",
    "# Send the prompt to the model with a stop word\n",
    "output = llm(prompt, max_tokens=2, stop=[\"Review\"]) \n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON responses with chat completion\n",
    "response_format = {\n",
    "  \"type\" : \"json_object\",\n",
    "  \"schema\" : {\n",
    "    \"type\" : \"object\",\n",
    "    \"properties\" : {\n",
    "      \"Product Name\" : { \"type\" : \"string\" },\n",
    "      \"Category\" : { \"type\" : \"string\" },\n",
    "      \"Sales Growth\" : { \"type\" : \"float\" }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "message_list = [\n",
    "  {         # System role defined as the market analyst\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a food industry market analyst. You analyze sales data and generate structured JSON reports of top-selling beverages.\"\n",
    "  },\n",
    "  {          # User role to pass the request\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Provide a structured JSON report of the top-selling beverages this year.\"\n",
    "  }\n",
    "]\n",
    "\n",
    "output = llm.create_chat_completion(\n",
    "  messages=message_list,\n",
    "  response_format=response_format\n",
    ")\n",
    "\n",
    "print(output[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Multi-Turn Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "  def __init__(self, llm:Llama, system_prompt='', history=[]):\n",
    "    self.llm = llm\n",
    "    self.system_prompt = system_prompt\n",
    "    self.history = [{\"role\": \"system\", \"content\": self.system_prompt}] + history\n",
    "  \n",
    "  def create_completion(self, user_prompt=''):\n",
    "    self.history.append({\"role\": \"user\", \"content\": user_prompt})  # Added input\n",
    "    output = self.llm.create_chat_completion(messages=self.history)\n",
    "    conversation_result = output[\"choices\"][0][\"message\"]\n",
    "    \n",
    "    self.history.append(conversation_result)  # Added response\n",
    "    return conversation_result[\"content\"] # Return the content of the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = Conversation(llm, system_prompt=\"You are a virtual travel assistant helping with planning trips.\")\n",
    "\n",
    "response1 = conversation.create_completion(\"What are some destinations in France for a short weekend break?\")\n",
    "print(f\"Response 1: {response1}\")\n",
    "\n",
    "response2 = conversation.create_completion(\"How about Spain?\")\n",
    "print(f\"Response 2: {response2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
