{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4ad48b7",
   "metadata": {},
   "source": [
    "The standard RAG workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5594f51",
   "metadata": {},
   "source": [
    "<image src=\"./images/standard_rag_workflow.png\" alt=\"RAG Workflow\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d505f8",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16591f2b",
   "metadata": {},
   "source": [
    "Preparing data for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e60e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loaders\n",
    "from langchain_core.document_loaders.base import CSVLoader\n",
    "csv_loader = CSVLoader(file_path = \"path/to/your/file.csv\")\n",
    "documents = csv_loader.load()\n",
    "print(documents)\n",
    "\n",
    "\n",
    "# Loading PDF files\n",
    "from langchain_core.document_loaders.base import PyPDFLoader\n",
    "pdf_loader = PyPDFLoader(file_path = \"path/to/your/file.pdf\")\n",
    "documents = pdf_loader.load()\n",
    "print(documents)\n",
    "\n",
    "\n",
    "# Loading html files\n",
    "from langchain_core.document_loaders.base import UnstructuredHTMLLoader\n",
    "html_loader = UnstructuredHTMLLoader(file_path = \"path/to/your/file.html\")\n",
    "documents = html_loader.load()\n",
    "first_document = documents[0]\n",
    "\n",
    "print(\"Content: \", first_document.page_content)\n",
    "print(\"Metadata: \", first_document.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c92cf",
   "metadata": {},
   "source": [
    "Perparing data for retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5d760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk Size\n",
    "#    - Big chunks are slow and difficult to interpret\n",
    "#    - Small chunks are fast but may lose context\n",
    "\n",
    "# Chunk Overlap\n",
    "#    - Overlapping chunks can help maintain context\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text = \"\"\"Machine learning is a fascinating field.\\n\\nIt involves algorithms and models that \n",
    "can learn from data. These models can then make predictions or decisions without being \n",
    "explicitly programmed to perform the task. \\nThis capability is increasingly valuable in \n",
    "various industries, from finance to healthcare. \\n\\nThere are many types of machine Learning, \n",
    "including supervised, unsupervised, and reinforcement learning. \\nEach type has its own strengths and applications. \"\"\"\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(text)\n",
    "print(chunks)\n",
    "print([len(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a95165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Text Splitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "print(chunks)\n",
    "print([len(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5d5484",
   "metadata": {},
   "source": [
    "Embedding and sotring the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe49f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sentence-transformers chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04289a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings (\n",
    "  model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "  documents=chunks, \n",
    "  embedding=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a43bde",
   "metadata": {},
   "source": [
    "Langchain expression language (LCEL) for RAG\n",
    "  - It is a declarative syntax for describing chains from prototypes to production\n",
    "  - It create modular retrieval pipelines which can combine retrieval and generation components together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b0841",
   "metadata": {},
   "source": [
    "<image src=\"./images/lcel.png\" alt=\"RAG Workflow\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c0aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma.from_documents(\n",
    "  documents=chunks, \n",
    "  embedding=embedding_model\n",
    ")\n",
    "\n",
    "# Instantiate a retriever\n",
    "retriever = vector_store.as_retriever(\n",
    "  search_type=\"similarity\", \n",
    "  search_kwargs={\"k\": 2}\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Creating a prompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "  Use the following pieces of context to answer the question at the end.\n",
    "  If you don't know the answer, say that you don't know.\n",
    "  Context: {context}\n",
    "  Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "# Building a lcel retrieval chain\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough() } # RunnblePassthrough allows you to pass the question directly to the chain\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"question\": \"What are the key findings or results presented in the paper?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d129cddc",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f69eb0",
   "metadata": {},
   "source": [
    "Extending the data retrieval with other files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b964517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading markdown files\n",
    "from langchain_core.document_loaders.base import UnstructuredMarkdownLoader\n",
    "markdown_loader = UnstructuredMarkdownLoader(file_path=\"path/to/your/file.md\")\n",
    "markdown_content = markdown_loader.load()\n",
    "print(markdown_content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e646006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading python files\n",
    "from langchain_core.document_loaders.base import PythonLoader\n",
    "python_loader = PythonLoader(file_path=\"path/to/your/file.py\")\n",
    "python_content = python_loader.load()\n",
    "print(python_content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba319b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the code files\n",
    "from langchain_text_splitters import Language\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "  language=Language.PYTHON,\n",
    "  chunk_size=150,\n",
    "  chink_overlap=10,\n",
    ")\n",
    "\n",
    "chunks = python_splitter.split_documents(python_data)\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "  print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf12c1",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a797d1b3",
   "metadata": {},
   "source": [
    "Advanced Splitting Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1672fb",
   "metadata": {},
   "source": [
    "<image src=\"./images/limitation of splitting.png\" alt=\"RAG Workflow\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9607c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting tokens\n",
    "import tiktoken\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "example_string = \"Mary had a little lamb, it's fleece was white as snow.\"\n",
    "\n",
    "encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "splitter = TokenTextSplitter(\n",
    "  encoding_name=encoding.name,\n",
    "  chunk_size=10, \n",
    "  chunk_overlap=2\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(example_string)\n",
    "\n",
    "for i, chunk in enumerate (chunks) :\n",
    "  print(f\"Chunk {i+1}: nNo. tokens: {len(encoding.encode(chunk))}\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff645ed",
   "metadata": {},
   "source": [
    "Semantic Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8612b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings \n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=\"...\", model='text-embedding-3-small')\n",
    "\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=embeddings, \n",
    "    breakpoint_threshold_type=\"gradient\",\n",
    "    breakpoint_threshold_amount=0.8\n",
    ")\n",
    "\n",
    "chunks = semantic_splitter.split_documents(document)\n",
    "print(chunks[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
