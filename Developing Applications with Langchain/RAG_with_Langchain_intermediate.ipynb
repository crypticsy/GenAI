{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4ad48b7",
   "metadata": {},
   "source": [
    "The standard RAG workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5594f51",
   "metadata": {},
   "source": [
    "<image src=\"./images/standard_rag_workflow.png\" alt=\"RAG Workflow\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d505f8",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16591f2b",
   "metadata": {},
   "source": [
    "Preparing data for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e60e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loaders\n",
    "from langchain_core.document_loaders.base import CSVLoader\n",
    "csv_loader = CSVLoader(file_path = \"path/to/your/file.csv\")\n",
    "documents = csv_loader.load()\n",
    "print(documents)\n",
    "\n",
    "\n",
    "# Loading PDF files\n",
    "from langchain_core.document_loaders.base import PyPDFLoader\n",
    "pdf_loader = PyPDFLoader(file_path = \"path/to/your/file.pdf\")\n",
    "documents = pdf_loader.load()\n",
    "print(documents)\n",
    "\n",
    "\n",
    "# Loading html files\n",
    "from langchain_core.document_loaders.base import UnstructuredHTMLLoader\n",
    "html_loader = UnstructuredHTMLLoader(file_path = \"path/to/your/file.html\")\n",
    "documents = html_loader.load()\n",
    "first_document = documents[0]\n",
    "\n",
    "print(\"Content: \", first_document.page_content)\n",
    "print(\"Metadata: \", first_document.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c92cf",
   "metadata": {},
   "source": [
    "Perparing data for retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5d760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk Size\n",
    "#    - Big chunks are slow and difficult to interpret\n",
    "#    - Small chunks are fast but may lose context\n",
    "\n",
    "# Chunk Overlap\n",
    "#    - Overlapping chunks can help maintain context\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text = \"\"\"Machine learning is a fascinating field.\\n\\nIt involves algorithms and models that \n",
    "can learn from data. These models can then make predictions or decisions without being \n",
    "explicitly programmed to perform the task. \\nThis capability is increasingly valuable in \n",
    "various industries, from finance to healthcare. \\n\\nThere are many types of machine Learning, \n",
    "including supervised, unsupervised, and reinforcement learning. \\nEach type has its own strengths and applications. \"\"\"\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(text)\n",
    "print(chunks)\n",
    "print([len(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a95165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Text Splitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "print(chunks)\n",
    "print([len(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5d5484",
   "metadata": {},
   "source": [
    "Embedding and sotring the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe49f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sentence-transformers chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04289a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings (\n",
    "  model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "  documents=chunks, \n",
    "  embedding=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a43bde",
   "metadata": {},
   "source": [
    "Langchain expression language (LCEL) for RAG\n",
    "  - It is a declarative syntax for describing chains from prototypes to production\n",
    "  - It create modular retrieval pipelines which can combine retrieval and generation components together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b0841",
   "metadata": {},
   "source": [
    "<image src=\"./images/lcel.png\" alt=\"RAG Workflow\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c0aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma.from_documents(\n",
    "  documents=chunks, \n",
    "  embedding=embedding_model\n",
    ")\n",
    "\n",
    "# Instantiate a retriever\n",
    "retriever = vector_store.as_retriever(\n",
    "  search_type=\"similarity\", \n",
    "  search_kwargs={\"k\": 2}\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Creating a prompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "  Use the following pieces of context to answer the question at the end.\n",
    "  If you don't know the answer, say that you don't know.\n",
    "  Context: {context}\n",
    "  Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "# Building a lcel retrieval chain\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough() } # RunnblePassthrough allows you to pass the question directly to the chain\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"question\": \"What are the key findings or results presented in the paper?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d129cddc",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f69eb0",
   "metadata": {},
   "source": [
    "Extending the data retrieval with other files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b964517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading markdown files\n",
    "from langchain_core.document_loaders.base import UnstructuredMarkdownLoader\n",
    "markdown_loader = UnstructuredMarkdownLoader(file_path=\"path/to/your/file.md\")\n",
    "markdown_content = markdown_loader.load()\n",
    "print(markdown_content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e646006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading python files\n",
    "from langchain_core.document_loaders.base import PythonLoader\n",
    "python_loader = PythonLoader(file_path=\"path/to/your/file.py\")\n",
    "python_content = python_loader.load()\n",
    "print(python_content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba319b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the code files\n",
    "from langchain_text_splitters import Language\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "  language=Language.PYTHON,\n",
    "  chunk_size=150,\n",
    "  chink_overlap=10,\n",
    ")\n",
    "\n",
    "chunks = python_splitter.split_documents(python_data)\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "  print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf12c1",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a797d1b3",
   "metadata": {},
   "source": [
    "Advanced Splitting Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1672fb",
   "metadata": {},
   "source": [
    "<image src=\"./images/limitation of splitting.png\" alt=\"RAG Workflow\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9607c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting tokens\n",
    "import tiktoken\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "example_string = \"Mary had a little lamb, it's fleece was white as snow.\"\n",
    "\n",
    "encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "splitter = TokenTextSplitter(\n",
    "  encoding_name=encoding.name,\n",
    "  chunk_size=10, \n",
    "  chunk_overlap=2\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(example_string)\n",
    "\n",
    "for i, chunk in enumerate (chunks) :\n",
    "  print(f\"Chunk {i+1}: nNo. tokens: {len(encoding.encode(chunk))}\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff645ed",
   "metadata": {},
   "source": [
    "Semantic Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8612b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings \n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=\"...\", model='text-embedding-3-small')\n",
    "\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=embeddings, \n",
    "    breakpoint_threshold_type=\"gradient\",\n",
    "    breakpoint_threshold_amount=0.8\n",
    ")\n",
    "\n",
    "chunks = semantic_splitter.split_documents(document)\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50f4c45",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8101295",
   "metadata": {},
   "source": [
    "Optimizing document retrieval\n",
    "- Dense : Encode chunks as a single vector with non-zero components. Pros: Capture semantic meaning. Cons: Computationally expensive.\n",
    "- Sparse : Encode using word matching with mostly zero components. Pros: Precise, explainable, rare-word handling Cons: Generalizability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba9e488",
   "metadata": {},
   "source": [
    "Sparse Retrieval Methods\n",
    "- TF-IDF : Encodes documents using the words that make the document unique\n",
    "- BM25 : Helps mitigate high-frequency words from saturating the encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ec4d56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant documents:\n",
      "Python was created by Guido van Rossum and released in 1991.\n"
     ]
    }
   ],
   "source": [
    "# If not already installed, install the langchain_community package\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "chunks = [\n",
    "  \"Python was created by Guido van Rossum and released in 1991.\",\n",
    "  \"Python is a popular language for machine learning (ML).\",\n",
    "  \"The PyTorch library is a popular Python library for AI and ML.\"\n",
    "]\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_texts(chunks, k=3)  # K is the number of top results to return\n",
    "results = bm25_retriever.get_relevant_documents(\"When was Python created?\")\n",
    "print(\"Most relevant documents:\")\n",
    "print(results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c12d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever.from_documents(\n",
    "  documents=chunks,\n",
    "  k=5\n",
    ")\n",
    "\n",
    "chain = (\n",
    "  {\"context\": retriever, \"question\": RunnablePassthrough() }\n",
    "  | prompt\n",
    "  | llm\n",
    "  | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke({\"question\": \"When was Python created?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa226e2e",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0524d85",
   "metadata": {},
   "source": [
    "Output accuracy: string evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d83019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "query = \"What are the main components of RAG architecture?\"\n",
    "predicted_answer = \"Training and encoding\"\n",
    "ref_answer = \"Retrieval and generation\"\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"You are an expert professor specialized in grading students' answers to\n",
    "You are grading the following question: {query}\n",
    "Here is the real answer: {answer}\n",
    "You are grading the following predicted answer: {result}\n",
    "Respond with CORRECT or INCORRECT:\n",
    "Grade: \n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  input_variables=[\"query\", \"answer\", \"result\"], \n",
    "  template=prompt_template\n",
    ")\n",
    "\n",
    "eval_llm = ChatOpenAI(temperature=0, model=\"gpt-40-mini\", openai_api_key='...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb596ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "\n",
    "qa_evaluator = LangChainStringEvaluator(\n",
    "  \"qa\",\n",
    "  config={\n",
    "    \"llm\" : eval_llm,\n",
    "    \"prompt\" : prompt    \n",
    "  }\n",
    ")\n",
    "\n",
    "score = qa_evaluator.evaluator.evaluate_strings(\n",
    "  prediction=predicted_answer,\n",
    "  reference=ref_answer,\n",
    "  input=query\n",
    ")\n",
    "\n",
    "print(f\"Score: {score}\")\n",
    "# Score 0 represents INCORRECT, 1 represents CORRECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7bc28d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe76e0d",
   "metadata": {},
   "source": [
    "RAGAS Framework : Faithfulness\n",
    "- Does the generated output faithfully represent the context?\n",
    "    - Faithfulness = No. of claims that can be inferred from the context / Total no. of claims in the output \n",
    "- Normalized to (0,1) -> 1 = highly relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAlEmbeddings \n",
    "from ragas.integrations.langchain import EvaluatorChain \n",
    "from ragas.metrics import faithfulness\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-40-mini\", api_key=\"...\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=\"...\")\n",
    "\n",
    "faithfulness_chain = EvaluatorChain (\n",
    "  metric=faithfulness,\n",
    "  llm=llm,\n",
    "  embeddings=embeddings\n",
    ")\n",
    "\n",
    "\n",
    "eval_result = faithfulness_chain({\n",
    "\"question\": \"How does the RAG model improve question answering with LLMs?\",\n",
    "\"answer\": \"The RAG model improves question answering by combining the retrieval of documents...\",\n",
    "\"contexts\": [\n",
    "  \"The RAG model integrates document retrieval with LLMs by first retrieving relevant passages...\",\n",
    "  \"By incorporating retrieval mechanisms, RAG leverages external knowledge sources, allowing the...\",\n",
    "]\n",
    "})\n",
    "\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c467e952",
   "metadata": {},
   "source": [
    "RAGAS Framework : Context Precision\n",
    "- How relevant are the retrieved documents to the query?\n",
    "- Normalized to (0,1) -> 1 = highly relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff6b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas. metrics import context_precision\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-40-mini\", api_key=\"...\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=\"...\")\n",
    "\n",
    "context_precision_chain = EvaluatorChain(\n",
    "  metric=context_precision,\n",
    "  llm=llm,\n",
    "  embeddings=embeddings\n",
    ")\n",
    "\n",
    "eval_result = context_precision_chain({\n",
    "\"question\": \"How does the RAG model improve question answering with large language models?\",\n",
    "\"ground_truth\": \"The RAG model improves question answering by combining the retrieval of...\",\n",
    "\"contexts\": [\n",
    "  \"The RAG model integrates document retrieval with LLMs by first retrieving...\",\n",
    "  \"By incorporating retrieval mechanisms, RAG leverages external knowledge sources...\"\n",
    "]\n",
    "})\n",
    "\n",
    "print(f\"Context Precision: {eval_result[ 'context_precision']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
