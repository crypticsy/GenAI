{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7170ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "experiment_name = \"My New Experiment\"\n",
    "\n",
    "# Create new experiment\n",
    "mlflow.create_experiment(experiment_name)\n",
    "\n",
    "# Tag new experiment\n",
    "mlflow.set_experiment_tag(\"scikit-learn\", \"lr\")\n",
    "\n",
    "# Set the experiment\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf21405",
   "metadata": {},
   "source": [
    "Training a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a run\n",
    "run = mlflow.start_run()\n",
    "\n",
    "# Print run info\n",
    "run.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd6ce89",
   "metadata": {},
   "source": [
    "Logging to MLflow tracking\n",
    "-  Metrics\n",
    "    - log_metric(\"accuracy\", 0.90)\n",
    "    - log_metrics({\"accuracy\": 0.90, \"loss\": 0.50})\n",
    "- Parameters\n",
    "  - log_param(\"n_jobs\", 1)\n",
    "  - log_params({\"n_jobs\": 1, \"fit_intercept\": False})\n",
    "- Artifacts\n",
    "  - log_artifact(\"file.py\")\n",
    "  - log_artifacts(\"./directory/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427bb0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training code here\n",
    "lr = LogisticRegression(n_jobs=1)\n",
    "\n",
    "# Moel Evaluation code here\n",
    "lr.fit(X, y)\n",
    "score = lr.score(X, y)\n",
    "\n",
    "# log a metric\n",
    "mlflow.log_metric(\"score\", score)\n",
    "\n",
    "# log a parameter\n",
    "mlflow.log_param(\"n_jobs\", 1)\n",
    "\n",
    "# log an artfiact\n",
    "mlflow.log_artifact(\"train_code.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddb0569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End a run\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4bb3a4",
   "metadata": {},
   "source": [
    "Open MLflow UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6109bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open MLflow Tracking UI\n",
    "mlflow ui\n",
    "\n",
    "# Go to: http://localhost:5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e94b3db",
   "metadata": {},
   "source": [
    "Searching runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0b0c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter string\n",
    "f1_score_filter = \"metrics.f1_score > 0.60\"\n",
    "\n",
    "# Search runs\n",
    "mlflow.search_runs(\n",
    "  experiment_name=[\"Insurance Experiment\"], \n",
    "  filter_string=f1_score_filter, \n",
    "  order_by=[\"metrics.precision_score DESC\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
